# EX√âCUTION S√âCURIS√âE DU CODE UTILISATEUR - ANALYSE APPROFONDIE

## üéØ PROBL√âMATIQUE

### D√©fis √† r√©soudre
1. **S√©curit√©** : Emp√™cher code malveillant (acc√®s filesystem, r√©seau, fork bomb, etc.)
2. **Isolation** : Chaque calcul ne doit pas affecter les autres
3. **D√©pendances** : Worker ne doit PAS avoir les d√©pendances utilisateur install√©es
4. **Performance** : Overhead minimal pour calculs courts
5. **Ressources** : Limiter CPU/RAM/temps par calcul

---

## üèóÔ∏è SOLUTIONS POSSIBLES (5 approches)

### **Approche 1 : Docker-in-Docker (DinD)** ‚≠ê RECOMMAND√â

**Concept** : Le worker lance un conteneur Docker temporaire pour chaque calcul

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Worker Container (Serverless/VM)       ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Worker Process (Python)          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Poll Redis                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Download code from S3          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Launch Docker container ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îº‚îÄ‚îÄ‚îê
‚îÇ  ‚îÇ  - Wait for completion            ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Upload results to S3           ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ
‚îÇ                                         ‚îÇ  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                                              ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  Execution Container (√©ph√©m√®re)          ‚îÇ
        ‚îÇ  - Image custom utilisateur              ‚îÇ
        ‚îÇ  - D√©pendances: numpy, scipy, Code_Aster ‚îÇ
        ‚îÇ  - CPU limit: 2 cores                    ‚îÇ
        ‚îÇ  - Memory limit: 4GB                     ‚îÇ
        ‚îÇ  - Network: disabled                     ‚îÇ
        ‚îÇ  - Filesystem: read-only sauf /tmp       ‚îÇ
        ‚îÇ  - Timeout: 5min                         ‚îÇ
        ‚îÇ                                           ‚îÇ
        ‚îÇ  $ python /tmp/user_code.py              ‚îÇ
        ‚îÇ  ‚Üí Execute code                          ‚îÇ
        ‚îÇ  ‚Üí Write output.json                     ‚îÇ
        ‚îÇ  ‚Üí Container destroyed                   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### **Code Worker avec Docker-in-Docker**

```python
# worker/docker_executor.py
import docker
import json
import tempfile
import time
from pathlib import Path

class DockerExecutor:
    """Execute user code in isolated Docker containers"""

    def __init__(self):
        self.docker_client = docker.from_env()

    def execute(self, task: dict) -> dict:
        """
        Execute user code in isolated container.

        task = {
            "task_id": "uuid",
            "code": "def calculate(inputs): ...",
            "inputs": {"param1": 10},
            "runtime": "python:3.12",  # ou custom image
            "requirements": ["numpy==1.24.0", "scipy==1.10.0"],
            "timeout": 300,
            "memory_limit": "2g",
            "cpu_limit": 2.0
        }
        """

        task_id = task['task_id']

        # 1. Cr√©er r√©pertoire temporaire pour le code
        with tempfile.TemporaryDirectory() as tmpdir:
            tmpdir_path = Path(tmpdir)

            # 2. √âcrire le code utilisateur
            code_file = tmpdir_path / "user_code.py"
            code_file.write_text(self._wrap_user_code(task['code']))

            # 3. √âcrire les inputs
            inputs_file = tmpdir_path / "inputs.json"
            inputs_file.write_text(json.dumps(task['inputs']))

            # 4. Cr√©er requirements.txt si besoin
            if task.get('requirements'):
                req_file = tmpdir_path / "requirements.txt"
                req_file.write_text('\n'.join(task['requirements']))

            # 5. Lancer container Docker avec restrictions
            try:
                container = self.docker_client.containers.run(
                    image=task.get('runtime', 'python:3.12-slim'),
                    command=self._get_execution_command(task),

                    # Volumes: mount code en read-only
                    volumes={
                        str(tmpdir_path): {
                            'bind': '/workspace',
                            'mode': 'ro'  # Read-only !
                        }
                    },

                    # Limits de ressources
                    mem_limit=task.get('memory_limit', '2g'),
                    cpu_quota=int(task.get('cpu_limit', 2.0) * 100000),
                    cpu_period=100000,

                    # S√©curit√©
                    network_mode='none',  # Pas d'acc√®s r√©seau !
                    read_only=True,       # Filesystem read-only
                    security_opt=['no-new-privileges'],
                    cap_drop=['ALL'],     # Drop toutes les capabilities

                    # Tmpfs pour /tmp (RAM disk)
                    tmpfs={'/tmp': 'size=512m,mode=1777'},

                    # Timeout
                    detach=True,
                    remove=False  # On nettoie manuellement apr√®s
                )

                # 6. Attendre la fin (avec timeout)
                result = container.wait(timeout=task.get('timeout', 300))

                # 7. R√©cup√©rer les logs (stdout/stderr)
                logs = container.logs(stdout=True, stderr=True).decode()

                # 8. R√©cup√©rer output.json depuis le container
                output_data = self._extract_output(container, '/tmp/output.json')

                # 9. Nettoyer le container
                container.remove(force=True)

                # 10. Retourner r√©sultat
                return {
                    'status': 'success' if result['StatusCode'] == 0 else 'error',
                    'output': output_data,
                    'logs': logs,
                    'exit_code': result['StatusCode']
                }

            except docker.errors.ContainerError as e:
                return {
                    'status': 'error',
                    'error': f"Container error: {str(e)}",
                    'logs': e.stderr.decode() if e.stderr else ''
                }

            except Exception as e:
                return {
                    'status': 'error',
                    'error': str(e)
                }

    def _wrap_user_code(self, user_code: str) -> str:
        """Wrap user code with safety harness"""

        wrapper = f'''
import json
import sys
import signal

# Timeout handler
def timeout_handler(signum, frame):
    raise TimeoutError("Execution timeout")

signal.signal(signal.SIGALRM, timeout_handler)
signal.alarm(300)  # 5min max

try:
    # Load inputs
    with open('/workspace/inputs.json', 'r') as f:
        inputs = json.load(f)

    # User code
{self._indent_code(user_code, 4)}

    # Execute calculate function
    result = calculate(inputs)

    # Write output
    with open('/tmp/output.json', 'w') as f:
        json.dump(result, f)

    sys.exit(0)

except Exception as e:
    error_output = {{"error": str(e), "type": type(e).__name__}}
    with open('/tmp/output.json', 'w') as f:
        json.dump(error_output, f)
    sys.exit(1)
'''
        return wrapper

    def _indent_code(self, code: str, spaces: int) -> str:
        """Indent user code"""
        indent = ' ' * spaces
        return '\n'.join(indent + line for line in code.split('\n'))

    def _get_execution_command(self, task: dict) -> list:
        """Get Docker command to execute"""

        commands = []

        # Install requirements if specified
        if task.get('requirements'):
            commands.append('pip install --no-cache-dir -r /workspace/requirements.txt')

        # Execute user code
        commands.append('python /workspace/user_code.py')

        return ['sh', '-c', ' && '.join(commands)]

    def _extract_output(self, container, file_path: str) -> dict:
        """Extract output file from container"""
        try:
            bits, stat = container.get_archive(file_path)

            # Extract tar archive
            import tarfile
            import io

            tar_stream = io.BytesIO(b''.join(bits))
            tar = tarfile.open(fileobj=tar_stream)

            # Read output.json
            output_file = tar.extractfile('output.json')
            return json.loads(output_file.read().decode())

        except Exception as e:
            return {'error': f'Failed to extract output: {str(e)}'}
```

#### **Avantages Docker-in-Docker**
- ‚úÖ **Isolation parfaite** : Kernel namespaces + cgroups
- ‚úÖ **D√©pendances custom** : Chaque user peut avoir son image
- ‚úÖ **Limits strictes** : CPU, RAM, r√©seau, filesystem
- ‚úÖ **Nettoyage automatique** : Container d√©truit apr√®s
- ‚úÖ **S√©curit√© prouv√©e** : Technologie mature

#### **Inconv√©nients**
- ‚ö†Ô∏è **Overhead** : ~2-5s pour spawn container (acceptable si calcul > 10s)
- ‚ö†Ô∏è **Complexit√©** : Besoin de Docker daemon sur le worker
- ‚ö†Ô∏è **Ressources** : ~200MB RAM par container

---

### **Approche 2 : gVisor (runsc)** ‚≠ê‚≠ê TR√àS S√âCURIS√â

**Concept** : Sandbox ultra-s√©curis√© avec syscall interception

gVisor = Runtime container de Google qui intercepte TOUS les syscalls

```python
# worker/gvisor_executor.py
import subprocess
import json

class GVisorExecutor:
    """Execute code with gVisor sandbox"""

    def execute(self, task: dict) -> dict:
        # 1. Cr√©er OCI bundle (config.json + rootfs)
        bundle_path = self._create_oci_bundle(task)

        # 2. Lancer avec runsc (gVisor runtime)
        result = subprocess.run(
            [
                'runsc',
                '--network=none',
                '--platform=ptrace',  # ou kvm pour meilleures perfs
                'run',
                '--bundle', bundle_path,
                task['task_id']
            ],
            capture_output=True,
            timeout=task.get('timeout', 300)
        )

        # 3. R√©cup√©rer r√©sultat
        return self._parse_output(result)
```

**Avantages gVisor** :
- ‚úÖ **S√©curit√© maximale** : Syscall interception
- ‚úÖ **Pas besoin VM** : Plus l√©ger que VMs
- ‚úÖ **Compatible Docker** : Drop-in replacement

**Inconv√©nients** :
- ‚ö†Ô∏è **Overhead** : 10-20% plus lent que Docker natif
- ‚ö†Ô∏è **Complexit√© setup** : Installer gVisor sur workers

---

### **Approche 3 : Firecracker MicroVMs** ‚≠ê‚≠ê‚≠ê ULTRA RAPIDE

**Concept** : MicroVMs ultra-l√©g√®res (AWS Lambda utilise √ßa)

```
Worker lance une MicroVM Firecracker par calcul
‚Üí Boot en 125ms
‚Üí Isolation kernel compl√®te
‚Üí Destroy apr√®s ex√©cution
```

```python
# worker/firecracker_executor.py
import requests
import json

class FirecrackerExecutor:
    """Execute code in Firecracker microVM"""

    def __init__(self):
        self.firecracker_socket = '/tmp/firecracker.sock'

    def execute(self, task: dict) -> dict:
        # 1. Configure microVM
        self._configure_vm(
            vcpu_count=task.get('cpu_limit', 2),
            mem_size_mib=task.get('memory_mb', 2048),
            kernel_image='/path/to/vmlinux',
            rootfs='/path/to/rootfs.ext4'
        )

        # 2. Boot microVM (125ms)
        self._boot_vm()

        # 3. Execute code via vsock
        result = self._execute_in_vm(task['code'], task['inputs'])

        # 4. Shutdown VM
        self._shutdown_vm()

        return result

    def _configure_vm(self, **config):
        """Configure Firecracker via API"""
        requests.put(
            f'http://localhost/machine-config',
            json={
                'vcpu_count': config['vcpu_count'],
                'mem_size_mib': config['mem_size_mib']
            }
        )
```

**Avantages Firecracker** :
- ‚úÖ **Boot ultra-rapide** : 125ms (vs 2-5s Docker)
- ‚úÖ **Isolation kernel compl√®te** : Vraie VM
- ‚úÖ **L√©ger** : 5MB RAM overhead
- ‚úÖ **Production-proven** : AWS Lambda, Fly.io

**Inconv√©nients** :
- üî¥ **Complexit√© √©lev√©e** : Setup non-trivial
- üî¥ **Linux only** : N√©cessite KVM
- üü° **Pas de support Windows/Mac**

---

### **Approche 4 : RestrictedPython** ‚ö†Ô∏è D√âCONSEILL√â SEUL

**Concept** : Sandbox Python natif (pas de conteneur)

```python
# worker/restricted_executor.py
from RestrictedPython import compile_restricted, safe_builtins
import RestrictedPython.Guards

def execute_user_code(code: str, inputs: dict) -> dict:
    """Execute in Python sandbox (NOT SECURE ENOUGH ALONE)"""

    # Compile code en mode restreint
    byte_code = compile_restricted(
        code,
        filename='<user_code>',
        mode='exec'
    )

    # Whitelist imports
    safe_globals = {
        '__builtins__': safe_builtins,
        '__name__': 'user_module',
        '__metaclass__': type,
        '_getattr_': RestrictedPython.Guards.safe_getattr,

        # Modules autoris√©s
        'numpy': __import__('numpy'),
        'scipy': __import__('scipy'),
        'math': __import__('math'),
    }

    # Execute
    exec(byte_code, safe_globals)

    # Call calculate()
    return safe_globals['calculate'](inputs)
```

**Avantages** :
- ‚úÖ **Tr√®s rapide** : Pas d'overhead
- ‚úÖ **Simple** : Pas de Docker/VM

**Inconv√©nients** :
- üî¥ **INSUFFISANT SEUL** : Bypasses possibles
- üî¥ **Pas de limite CPU/RAM** : Doit √™tre combin√©
- üî¥ **Imports limit√©s** : Difficile de whitelister tout

**‚Üí Peut √™tre utilis√© EN COMBINAISON avec Docker** pour double protection

---

### **Approche 5 : Subprocess avec ulimit** ‚ö†Ô∏è FAIBLE ISOLATION

```python
# worker/subprocess_executor.py
import subprocess
import resource

def execute_user_code(code: str, inputs: dict) -> dict:
    """Execute in subprocess with resource limits (WEAK)"""

    def set_limits():
        # Limite CPU: 5min
        resource.setrlimit(resource.RLIMIT_CPU, (300, 300))

        # Limite m√©moire: 2GB
        resource.setrlimit(resource.RLIMIT_AS, (2*1024*1024*1024, 2*1024*1024*1024))

        # Limite processus
        resource.setrlimit(resource.RLIMIT_NPROC, (1, 1))

    result = subprocess.run(
        ['python', '-c', code],
        input=json.dumps(inputs),
        capture_output=True,
        timeout=300,
        preexec_fn=set_limits  # Linux only
    )

    return json.loads(result.stdout)
```

**‚Üí NE PAS UTILISER en production** : Isolation insuffisante

---

## üìä COMPARAISON DES APPROCHES

| Approche | S√©curit√© | Performance | Complexit√© | Isolation deps | Co√ªt dev |
|----------|----------|-------------|------------|----------------|----------|
| **Docker-in-Docker** | ‚≠ê‚≠ê‚≠ê‚≠ê | üü° (2-5s overhead) | üü¢ Moyenne | ‚úÖ Parfaite | üü¢ 1 semaine |
| **gVisor** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üü° (10-20% slower) | üü° Moyenne | ‚úÖ Parfaite | üü° 2 semaines |
| **Firecracker** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ (125ms boot) | üî¥ √âlev√©e | ‚úÖ Parfaite | üî¥ 1 mois |
| **RestrictedPython** | ‚≠ê‚≠ê | ‚úÖ (natif) | üü¢ Simple | ‚ùå Partag√©e | üü¢ 3 jours |
| **Subprocess** | ‚≠ê | ‚úÖ (natif) | üü¢ Simple | ‚ùå Partag√©e | üü¢ 1 jour |

---

## üéØ RECOMMANDATION FINALE

### **Pour d√©marrer (MVP) : Docker-in-Docker** ‚úÖ

**Pourquoi** :
1. ‚úÖ **Bon compromis** s√©curit√©/performance/complexit√©
2. ‚úÖ **Isolation parfaite** des d√©pendances
3. ‚úÖ **Technologie mature** : Docker = production-proven
4. ‚úÖ **Facile √† debugger** : `docker logs`, `docker inspect`
5. ‚úÖ **√âvolutif** : Peut migrer vers Firecracker plus tard

**Setup Worker avec Docker** :

```dockerfile
# Dockerfile du Worker
FROM python:3.12-slim

# Installer Docker CLI
RUN apt-get update && apt-get install -y \
    docker.io \
    && rm -rf /var/lib/apt/lists/*

# Installer d√©pendances worker
RUN pip install redis boto3 docker

COPY worker/ /app/
WORKDIR /app

CMD ["python", "worker.py"]
```

**D√©ploiement** :
```bash
# Sur Scaleway Serverless Containers
scw container container create \
  --name mecapy-worker \
  --privileged true \  # N√©cessaire pour Docker-in-Docker
  --registry-image mecapy/worker:v1

# OU sur VMs
docker run -d \
  -v /var/run/docker.sock:/var/run/docker.sock \  # Socket Docker
  mecapy-worker:v1
```

---

## üî• ARCHITECTURE RECOMMAND√âE FINALE

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  API FastAPI (Clever Cloud)                           ‚îÇ
‚îÇ  - Enqueue job dans Redis                             ‚îÇ
‚îÇ  - Upload code/inputs vers S3                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                           ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Redis Queue     ‚îÇ       ‚îÇ  PostgreSQL     ‚îÇ
‚îÇ  - mecapy:jobs   ‚îÇ       ‚îÇ  - Tasks meta   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  WORKER POOL (5 Serverless Containers ou VMs)        ‚îÇ
‚îÇ                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ Worker Process                              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ 1. Poll Redis (blpop)                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ 2. Download code/inputs from S3             ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ 3. Launch Docker container ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îê
‚îÇ  ‚îÇ 4. Wait completion                          ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ 5. Upload result to S3                      ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ 6. Update Redis/PostgreSQL                  ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                                                            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  EXECUTION CONTAINER (√©ph√©m√®re, d√©truit apr√®s)            ‚îÇ
        ‚îÇ                                                            ‚îÇ
        ‚îÇ  Image: python:3.12 (ou custom avec numpy/scipy)          ‚îÇ
        ‚îÇ  Limits:                                                   ‚îÇ
        ‚îÇ    - CPU: 2 cores                                          ‚îÇ
        ‚îÇ    - RAM: 4GB                                              ‚îÇ
        ‚îÇ    - Network: DISABLED                                     ‚îÇ
        ‚îÇ    - Filesystem: READ-ONLY (sauf /tmp)                     ‚îÇ
        ‚îÇ    - Timeout: 5min                                         ‚îÇ
        ‚îÇ                                                            ‚îÇ
        ‚îÇ  Security:                                                 ‚îÇ
        ‚îÇ    - no-new-privileges                                     ‚îÇ
        ‚îÇ    - cap-drop=ALL                                          ‚îÇ
        ‚îÇ    - User: non-root                                        ‚îÇ
        ‚îÇ                                                            ‚îÇ
        ‚îÇ  Execution:                                                ‚îÇ
        ‚îÇ    $ pip install -r requirements.txt  # si n√©cessaire      ‚îÇ
        ‚îÇ    $ python /workspace/user_code.py                        ‚îÇ
        ‚îÇ    ‚Üí Write /tmp/output.json                                ‚îÇ
        ‚îÇ    ‚Üí Container destroyed                                   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîí S√âCURIT√â MULTICOUCHE

### **Couche 1 : Validation API**
```python
# api/validation.py
def validate_user_code(code: str) -> bool:
    """Validation statique du code"""

    # Blacklist imports dangereux
    forbidden = ['os', 'subprocess', 'sys', 'socket', '__import__']
    for module in forbidden:
        if f'import {module}' in code:
            raise SecurityError(f"Forbidden module: {module}")

    # Taille max code
    if len(code) > 100_000:  # 100KB
        raise ValueError("Code too large")

    return True
```

### **Couche 2 : RestrictedPython (optionnel, d√©fense en profondeur)**
```python
# worker/restricted_wrapper.py
from RestrictedPython import compile_restricted

def wrap_code(user_code: str) -> str:
    """Double validation avec RestrictedPython"""

    # Tenter compilation restreinte
    compile_restricted(user_code, '<string>', 'exec')

    # Si OK, retourner code wrapped
    return user_code
```

### **Couche 3 : Docker isolation (principal)**
- Network disabled
- Filesystem read-only
- Capabilities dropped
- Cgroups limits

### **Couche 4 : Monitoring**
```python
# worker/monitor.py
def monitor_execution(container):
    """Monitor container pour d√©tection anomalies"""

    stats = container.stats(stream=False)

    # Check CPU spike
    if stats['cpu_stats']['cpu_usage']['total_usage'] > THRESHOLD:
        container.kill()

    # Check memory
    if stats['memory_stats']['usage'] > MEMORY_LIMIT:
        container.kill()
```

---

## üí∞ CO√õTS OVERHEAD DOCKER

### **Par calcul** :
- Spawn container : 2-5s
- Execution : Variable (10s-5min)
- Destroy : 0.5s
- **Total overhead : 2.5-6s**

**‚Üí Acceptable si calcul > 10s** (overhead < 30%)

### **Optimisation possible** :
```python
# Pool de containers pr√©-cr√©√©s (optionnel)
class ContainerPool:
    def __init__(self, size=3):
        self.pool = [
            docker_client.containers.create('python:3.12')
            for _ in range(size)
        ]

    def get_container(self):
        """R√©utiliser container au lieu de cr√©er"""
        container = self.pool.pop()
        container.restart()
        return container
```

**‚Üí R√©duit overhead √† ~500ms** mais complexit√© accrue

---

## ‚úÖ PLAN D'IMPL√âMENTATION

### **Semaine 1 : Proof of Concept**
```bash
# Test Docker executor localement
python test_docker_executor.py

# V√©rifier isolation
- Code malveillant bloqu√© ?
- Limits CPU/RAM respect√©es ?
- Timeout fonctionne ?
```

### **Semaine 2 : Int√©gration Worker**
```python
# worker/worker.py
from docker_executor import DockerExecutor

executor = DockerExecutor()

while True:
    job = redis.blpop('mecapy:jobs')
    result = executor.execute(job)
    upload_to_s3(result)
```

### **Semaine 3 : Tests de charge**
```bash
# 100 calculs simultan√©s
python benchmark.py --concurrent 100
```

### **Semaine 4 : Production**
```bash
# Deploy 5 workers
./deploy_workers.sh 5
```

---

## üìã ALTERNATIVE SI DOCKER-IN-DOCKER IMPOSSIBLE

### **Option : Workers d√©di√©s par runtime**

```
Worker Pool A (3 workers) : Python + NumPy/SciPy
Worker Pool B (2 workers) : Python + Code_Aster
Worker Pool C (2 workers) : Python + Custom deps

‚Üí API route selon requirements
```

**Avantages** :
- ‚úÖ Pas besoin Docker-in-Docker
- ‚úÖ D√©pendances pr√©-install√©es (plus rapide)

**Inconv√©nients** :
- ‚ùå Moins flexible (limit√© aux runtimes pr√©d√©finis)
- ‚ùå Maintenance de plusieurs images

---

**‚úÖ VERDICT : Docker-in-Docker = Meilleur compromis pour MVP**

**Document g√©n√©r√© le** : 2025-09-30
**Version** : 1.0 - Analyse ex√©cution s√©curis√©e
